base_model: Qwen/Qwen2.5-VL-7B-Instruct  # Replace with the model's actual path
model_type: Qwen2_5_VLForConditionalGeneration
tokenizer: Qwen/Qwen2.5-VL-7B-Instruct  # Tokenizer should match the model
tokenizer_type: AutoTokenizer
dataset:
  path: "processed_dataset"
  format: "custom"  # Using custom format (defined below)
  image_column: "image"
  question_column: "question"
  answer_column: "answer"
train:
  batch_size: 1
  gradient_accumulation_steps: 16
  epochs: 3
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 100
peft:
  method: "lora"  # Use LoRA for parameter-efficient fine-tuning
  lora_r: 16
  lora_alpha: 16
  lora_dropout: 0.05
trainer:
  type: "deepspeed"  # Choose from "deepspeed" or "accelerate"
#  deepspeed_config: "path/to/deepspeed_config.json"  # Optional, for advanced distributed training
logging:
  log_interval: 50
  log_to_wandb: true
  wandb_project: "qwen-vl-finetuning"
output_dir: "./qwen2.5-vl-finetuned"
save_interval: 500
